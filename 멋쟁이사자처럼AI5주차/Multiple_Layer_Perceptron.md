# 다층 퍼셉트론

  : 신경망은 입력 벡터를 시작으로 중간 표현을 거쳐 출력 벡터를 얻어내는 신경망 구조
  
  ex) 다중 퍼셉트론 예시
  
  ![20210405191043](https://user-images.githubusercontent.com/59636424/113563483-bd210d80-9642-11eb-81af-ea62209806c0.png)
  
## 은닉 계층
  
  : 출력에 직접적으로 들어나지 않고 은닉 벡터라고도 표현한다.
    
  : 각 계층안의 퍼셉트론끼리는 서로 어떠한 연결이 없기에 영향을 주고 받지 않는다.
    
  : 앞 계층의 출력이 뒤 계층의 모든 퍼셉트론에 공통 입력으로 제공
    
## 완전 연결 방식
  
  : 다층 퍼셉트론의 인접 계층끼리는 방향성을 갖는다.(노드들이 완전 연결 계층)
  
## 오차 역전파법

  : 순전파 방식을 거꾸로 돌려 신경망의 출력값과 실제 정답 사이의 오차를 먼저 구하여 바로 앞 단계의 계층으로 거슬로 올라가며 가중치를 조정하는 방법
  
  - 출력값의 오차를 기반으로 출력층에서 입력층 방향으로 weight(가중치)와 bias를 거꾸로 갱신
  
  - 미분할 때 다량의 계산에 대한 여러움으로 손실함수의 기울기를 더 쉽게 구하기 위한 방법

## 역전파 흘러가는 과정(손실 함수 구하는 과정)

  ![다층퍼셉트론1](https://user-images.githubusercontent.com/59636424/113565042-52bd9c80-9645-11eb-9b47-5d1d8b77842b.png)
  
  : 마지막 계층의 손실 함수 델타k라고 가정, (델타k(마지막 계층) x wk(마지막 은닉 계층과 출력 계층 사이의 가중치)) x 활성화 함수 미분(f'(xk))
  
  -> 마지막 이전 계층의 델타 k+1값 성립
  
  ![다층퍼셉트론2](https://user-images.githubusercontent.com/59636424/113566637-0cb60800-9648-11eb-88d2-c74b108d5d82.png)
  
  - 은닉 계층의 폭: 해당 은닉 계층이 갖는 퍼셉트론 수
 
## 비선형 활성화 함수
  
  : 일차 함수로 표현이 불가능한 좀 더 복잡한 기능을 수행하여 선형성의 한계를 벗어나는 역할을 한다.
  
  
## ReLU 함수

![relu](https://user-images.githubusercontent.com/59636424/113567025-c319ed00-9648-11eb-8e72-22a536f8ee1b.png)

- 소프트 맥스의 경우는 벡터 원소들을 한데 묶어 처리하기에 은닉 계층 출력 처리에 적합하지 않는다.

- 미분할 경우 x=0 지점이 미분이 불가능하여 np.sign()함수를 통해 ReLU함수 미분을 표현한다.



  

  
    
