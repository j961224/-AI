{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터 크롤링.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxO1ASOEO_ny"
      },
      "source": [
        "# 데이터 크롤링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkRBJKq6PGMu"
      },
      "source": [
        "web crawler: 웹 페이지의 데이터를 모아주는 소프트웨어\n",
        "\n",
        "web crawling: 크롤러를 사용해 웹 페이지의 데이터를 추출해내는 행위\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XKZQ5YQPPu"
      },
      "source": [
        "**크롤링으로 실시간 검색어 순위 보여주고 파일 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a6Zv9-CPBV_"
      },
      "source": [
        "from bs4 import BeautifulSoup #BeautifulSoup는 모듈이 아니라 기능이므로 앞에 모듈 이름 bs4를 적어준 것이다.\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "url = \"http://www.daum.net/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "rank = 1\n",
        "\n",
        "# a tag 중에 link_favorsch 부분 추출\n",
        "results = soup.findAll('a','link_favorsch')\n",
        "\n",
        "#open(\"파일.확장자\",r or w -> 기존 내용 보존하고 덧붙이는 것 or a(append) -> 기존 파일에 덮어쓰는 것)\n",
        "#open에 append모드로 할 시, 계속 update 덧붙여서 감\n",
        "search_rank_file = open(\"rankresult.txt\",\"a\")\n",
        "\n",
        "print(datetime.today().strftime(\"%Y년 %m월 %d일의 실시간 검색어 순위입니다.\\n\"))\n",
        "\n",
        "for result in results:\n",
        "    search_rank_file.write(str(rank)+\"위:\"+result.get_text()+\"\\n\")\n",
        "    #result.get_text()를 통해 실검 순위 키워드 가져오기\n",
        "    print(rank,\"위 : \",result.get_text(),\"\\n\")\n",
        "    rank += 1\n",
        "\n",
        "#계속 실행하면 계속 1~15위가 아래에 붙어서 나옴!!\n",
        "\n",
        "\n",
        "#response.url = 응답값의 주소를 의미\n",
        "#response.content = 응답값 화면의 UI 구성 요소들의 구조와 내용을 읨\n",
        "#response.encoding = 응답값의 문자 인코딩 형식을 의미\n",
        "#response.headers = 응답값의 헤더값을 의미\n",
        "#response.json = json의 응답값을 의미\n",
        "#response.links = 응답값에서 연결되는 링크를 의미\n",
        "#response.ok = 응답이 정상적으로 작동하는지 확인\n",
        "#response.status_code = 응답값의 현재 상태 코드를 의미"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IcUm_XXPreW"
      },
      "source": [
        "pip install requests: 모듈 설치할 시(외부 모듈) 사용\n",
        "\n",
        "BeautifulSoup: 데이터를 가져와서 의미있는 데이터로 변형해주는 기능\n",
        "\n",
        "BeautifulSoup(response.text,'html.parser'): 어떤 통에 정보를 담아주는 기능인 BeautifulSoup로 response.text(문자열 타입)을 beautifulSoup type으로 변경\n",
        "\n",
        "\n",
        "parsing: 데이터를 의미있게 분해하는 방법 -> 파싱을 도와주는 것이 parser(ex. html.parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkagNx4PWtPs"
      },
      "source": [
        "**header를 통한 크롤링 봇이 아님을 알리고 크롤링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kh2K9SMQG_q"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'} #내가 크롤링 봇이 아니다는 증표\n",
        "url = \"https://datalab.naver.com/keyword/realtimeList.naver?age=20s\"\n",
        "response = requests.get(url,headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "rank = 1\n",
        "# span - item_title\n",
        "results = soup.findAll('span','item_title')\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "search_rank_file = open(\"rankresult.txt\",\"a\")\n",
        "\n",
        "print(datetime.today().strftime(\"%Y년 %m월 %d일의 실시간 검색어 순위입니다.\\n\"))\n",
        "\n",
        "for result in results:\n",
        "    search_rank_file.write(str(rank)+\"위:\"+result.get_text()+\"\\n\")\n",
        "    print(rank,\"위 : \",result.get_text(),\"\\n\")\n",
        "    rank += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}