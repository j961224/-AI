# 자연어 처리

 : 일상에서 사용하는 언어릴 자연어라고 칭하며 컴퓨터가 이를 분석하여 처리할 수 있도록 하는 일이다.

## 토큰(token)

 : 가장 기본이 되는 단어를 칭하며 토크나이징(tokenizing) 방법에 따라 달라질 수 있지만 일반적으로 의미가 있는 가장 작은 정보 단위로 결정된다.
 
 - 토크나이징
 
  : 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업(텍스트 전처리 과정에 주로 사용)
  
  - **KoNLPy(코엔엘파이)**
  
    : 대표적인 한국어 토크나이징을 지원하는 파이썬 모듈(형태소를 토큰 단위로 사용)
  
  - 형태소
  
    : 일정한 의미가 있는 가장 작은 말의 단위(더 이상 쪼개지지 않는 단어)
    
    : 한국어는 용언에 따라 여러 가지 의미를 붙기에 띄어쓰기만으로는 토크나이징 할 수 없다.
    
    : 그래서 문장에서 형태소를 추출하면서 형태소의 뜻과 문맥을 고려해 품사 태깅(KoNLPy에서 pos가 그 역할을 한다.)
  
  - KoNLPy 패키지 이용법: https://konlpy-ko.readthedocs.io/ko/v0.4.3/

## Kkma

 : konlpy.tag 패키지 중 하나의 모듈로 자연어 처리를 위한 한국형 형태소 분석기
 
 |함수|설명|
 |----|-----|
 |morphs(phrase)|인자로 입력한 문장을 형태소 단위로 토크나안징 한다. 이들은 리스트 형태로 반환|
 |nouns(phrase)|인자로 입력한 문장에서 품사가 명사은 토큰만 추출|
 |pos(phrase,flatten=True)|POS tagger라고도 하며, 인자로 입력한 문장에서 형태소 추출 후, 품사를 태깅하여 튜플 형태로 묶어 리스트로 반환|
 |setences(phrase)|인자로 입력한 여러 문장을 분리해주는 역할, 이 또한 리스트로 변환|
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  - 주로 Kkma 사용 시, 한글 형태소 품사 태그표 주로 참조: http://kkma.snu.ac.kr/documents/?doc=postag

## Komoran
  
  : 자바로 개발한 한국어 형태소 분석기로 다른 형태소 분석기와 다르게 공백이 포함된 형태소 단위로도 분석 가능
  
  : 위의 Kkma의 morphs, nouns, pos 함수를 사용한다.
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  - Komoran은 Kkma보다 형태소를 빠르게 분석해 다양한 품사 태그를 지원한다.
   
   : https://www.shineware.co.kr/products/komoran/#demo?utm_source=komoran-kr&utm_medium=Referral&utm_campaign=github-demo
   
   (실습은 NLP_basic.ipynb에서 참조)


## Okt

  : Twitter 한국어 처리기에서 파생된 한국어 처리기로 띄어쓰기가 어느 정도 되어 있는 문장을 빠르게 분석할 때, 자주 사용된다.
  
  앞의 Komoran 패키지의 함수 3개를 지원하고 2개를 더 지원합니다.
  
  |함수|설명|
  |----|----|
  |normalize(phrase)|입력한 문장을 정규화시킨다. (정규화 이전: 사랑햌ㅋㅋ -> 정규화 이후: 사랑해ㅋㅋ|
  |phrases(phrase)|입력한 문장에서 어구룰 추출한다. (입력: 오늘 날씨가 좋아요. -> 출럭: ['오늘', '오늘 날씨','날씨'])|
  
  - Okt는 앞의 형태소 분석기보다 품사 정보는 작지만 분석 속도가 빠르고 normalize함수를 통해 오타가 섞인 문장을 정규화 해준다.
  
  (실습은 NLP_basic.ipynb에서 참조)
 
## 챗봇

  : 챗봇의 입력 데이터는 보통 **인터넷 구어체**가 많다. 따라서 새롭게 생겨나는 단어나 문장은 형태소 분석기가 인식을 못하는 경우가 많다. 그러므로 기존의 많은 문장을 기반으로 학습하여 형태소 분석기를 개발했기에 **새로운 형태의 단어와 문장은 인식률 저하의 원인**이 된다.
  
  -> 이를 해결하기 위해서 형태소 분석기에서 인식하지 못하는 단어들을 직접 추가하는 방법을 살펴본다.
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  : 실습에서는 Komoran을 사용했지만 Kkma나 Okt를 사용해도 무관

## Kkma, Okt, Komoran 형태소 분석기 정리

: 이때까지 한국어 토크나이징을 알아보았다. 영어는 단순히 토큰 정보만 필요하다면 띄어쓰기만 하더라도 좋은 결과를 보여준다.

  -> 하지만 **한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러 가지 어미를 붙이기에 띄어쓰기만으로는 토크나이징 불가**
  
  -> 그래서 KoNLPy 형태소 분석기를 이용해 형태소 단위의 토큰과 품사 정보를 추출했고 이에 **필요 없는 정보를 제거하는 과정을 추가**해야한다!
  
  -> 이를 **전처리**라고 한다.
  
  ![형태소분석기](https://user-images.githubusercontent.com/59636424/115201639-84982e00-a130-11eb-8e1d-9529f05c6346.png)

## 임베딩

  : 자연어를 숫자나 벡터 형태로 변환 -> 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정
  
  - 문장 임베딩
  
   : 문장 전체를 벡터로 표현하는 방법으로 전체 문장의 흐름을 파악해 벡터로 변환하기에 문맥적 의미를 지니는 장점이 있다. 하지만 임베딩하기 위해 많은 문장 데이터가 필요하고 학습 비용이 많이 든다.
   
   - 단어 임베딩
   
   : 말뭉치에서 각각의 단어를 벡터로 변환하는 방법
   
   -> 동음 이의어에 대한 구분을 하지 않기에 의미가 다르더라도 단어 형태가 같다면 동일한 벡터값을 부여한다. (ex. 과일 사과, 미안할 때 사과)
   
   - **원-핫 인코딩**
   
   : 단어를 숫자 벡터로 변환하는 가장 기본적인 방법으로 요소들 중 단 하나의 값만 1이고 나머지 요소값은 0으로 인코딩힌다.
   
   -> 전체 요소 중 단 하나의 값만 1이기에 희소 벡터라고도 한다. (ex. ([1,0,0],[0,1,0],[0,0,1] 표현)
   
   * 원-핫 인코딩을 수행 시, 단어 집합인 **사전**을 먼저 만들어야한다. (서로 다른 모든 단어의 집합)
   
   -> 이로써, 100단어면 100차원으로 사전 내 단어 순서대로 고유한 인덱스 번호를 부여한다.
   
   (실습은 NLP_basic.ipynb에서 참조)
   
   * 희소 벡터는 각각의 차원이 독립적으로 되어 있어 직관적으로 해석이 가능하지만 메모리가 많이 사용되고 단어 간의 연관성을 알 수 없다.
   
   -> 이를 해결하기 위해서 각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 **분산 표현**을 고안했다.
   
     ex) 색상을 표현하는 방식 예시
   
      : RGB모델은 3차원 형태의 벡터로 생각할 수 있으며 분산 표현 방식
   
   -> 분산 표현 시, 임베딩 벡터의 모든 차원에 의미 있는 데이터를 고르게 밀집
   
   -> 이로써, 데이터 손실을 최소화하며 벡터 차원이 압축되는 효과가 생긴다. (여러 차원에 분산되어 표현)
   
   -> 원하는 차원에 데이터를 최대한 밀집시킬 수 있는 **밀집 표현**으로 벡터 압축 효과가 생긴다.
   
   * 단어 많으면 회소 표현 방식을 사용 할 시, 너무 많은 차원이 필요하고 비효율적이게 되어 학습이 어려워지는 **차원의 저주** 문제가 발생한다.
   
     -> 분산 표현 방식으로 임베딩 벡터 차원을 데이터 손실 최소화시키므로 차원의 저주 발생 확률을 줄인다.
   
   * 임베딩 벡터는 단어의 의미, 주변 단어간의 관계 등 많은 정보가 표현되어 있어 일반화 능력이 존재한다.
   
   ex) 희소 표현 방식 시, '남자'와 '남성'의 관계는 전혀 없다. 하지만 분산 표현 방식 사용 시, 유사한 의미를 갖는 단어들은 '남자'와 '남성'의 단어 위츠는 매우 가깝게 표현된다.
   
   ![분산표현기법장점1](https://user-images.githubusercontent.com/59636424/115204615-a5ae4e00-a133-11eb-855c-cb8bdb1e2186.png)

## Word2Vec

  : 대표적인 단어 임베딩 기법
  
  -> 해당 단어를 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치
  
  -> 방향성에 대해 이해와 연산을 통해 단어간 관계를 계산 할 수 있다.
  
  1. CBOW(continuous bag-of words)모델
  
   : 맥락이라 표현되는 주변 단어(앞뒤 단어)들을 이용해 타겟 단어를 예측하는 신경망 모델
   
   -> 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용
   
   -> 손실만 계산하면 되므로 학습 속도가 빠르다.
   
   * Window: 앞뒤로 몇 개의 단어까지 확인할지 결정
  
  2. skip-gram 모델
  
   : 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델
   
   -> CBOW모델에 비해 예측해야 하는 맥락이 많다.
   
   -> CBOW모델에 비해 임베딩 품질이 우수
   
   -> 확률 분포를 가지고 주변 단어를 예측
  
 - 임베딩 모델링과 자연어 처리를 위한 Gensim 패키지 주로 사용
  
  (ex. Movie_review_Word2Vec.ipynb)
    
## Text Similarity

  : 문장 간에 의미가 얼마나 유사한지 계산
  
  - 챗봇 엔진은 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사한지 계산할 수 있어햐 적절한 답변을 출력한다.

  1. 인공신경망의 Word2Vec
  
  2. 통계적 방법의 n-gram
  
   : 주어진 문장에서 n개의 연속적인 단어 나열을 의미한다. 한 마디로, n개의 단어를 토큰으로 사용한다.
   
   -> 이는 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법이다.
   
   -> 이렇게 토큰을 분리 한 후에 단어 문서 행렬을 만들고 두 문장을 비교해 동일한 단어의 출현 빈도를 확률적으로 계산한다.
   
   ![oh](https://user-images.githubusercontent.com/59636424/115366202-128f1a00-a200-11eb-8c5a-e041e0a04825.png)
   
   : 이렇게 n에 따라 토큰으로 분리 할 수 있다.
   
   ![ho1](https://user-images.githubusercontent.com/59636424/115366430-4b2ef380-a200-11eb-80af-ce1508e493e6.png)
   
   : n-gram으로 토큰을 분리한 후에 이와 같이 단어 문서 행렬(TDM)을 만들 수 있다.
   
   ![h02](https://user-images.githubusercontent.com/59636424/115366616-71ed2a00-a200-11eb-88ca-509fcac1aa03.png)
   
   (tf는 두 문장 A와 B에서 동일한 토큰의 출현 빈도, tokens는 해당 문자에서 전체 토큰 수를 의미)
   
   : 이 식을 통해 유사도를 구하는데 1.0에 가까울수록 유사도가 크다.
   
   -> 위와 같은 단어 문서 행렬의 유사도는 4/6으로 66% 유사하다.
   
  3. 통계적 방법의 코사인 유사도
  
   : 단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성을 파악
   
   -> 여기서는 벡터의 크기가 중요하지 않을 때, 거리를 측정하기 위해 사용되어 크기 상관없이 결과가 안정적이다.
   
   -> 그래서 단어의 출현 빈도를 통해 유사도를 계산할 시, 동일한 단어가 많을수록 벡터의 크기가 커진다.
   
   ![ho3](https://user-images.githubusercontent.com/59636424/115367334-14a5a880-a201-11eb-8bb7-028ce38bb3a2.png)
   
   : 이 코사인 유사도는 -1~1값으로 1일 경우, 두 벡터의 방향이 동일하다. -1일 경우, 반대의 방향이고 0일경우, 두 벡터는 직각을 이룬다.
   
   -> 벡터의 방향이 같아질수록 유사하다.
   
   -> 위의 단어 문서 행렬을 코사인 유사도로 계산하면 5/6으로 83%의 유사성을 띈다.
  
  (ex. Text_similarity_Algorithm.ipynb)

## CNN

 : 문장 의도 분류를 위해 사용
 
 -> 수치(벡터) 표현 가능한 대상이면 특징을 뽑아내도록 CNN 모델을 학습
 
 (ex. chatbot_classification.ipynb)
 
 - 문장을 단어 시퀀스로 토큰화시키고 벡터로 만들면 문장 길이가 제각각이므로 벡터 크기가 다르다.
 
  : CNN은 입력 계층은 고정된 개수의 입력모드이므로 시퀀스 번호로 전체 벡터 크기를 동일하게 맞춰야 한다.
  
  -> maxlen으로 padding을 이용해 0으로 채운다.
  
  -> 최대 몇 개의 단어 토큰으로 구성되어 있는지 파악하여 패딩해야 한다!
  
    -> 너무 크게 maxlen을 잡으면 빈 공간이 많아 자원의 낭비가 생기고 너무 작게 잡으면 입력데이터가 손상되는 상황 발생
  
## 양방향 LSTM(Bi-LSTM)

 : 챗봇 언젠에서 개체명 인식을 위해서 자주 사용되며 BIO 표기법을 사용합니다.
 
 ![z1](https://user-images.githubusercontent.com/59636424/115550549-f0bb9300-a2e4-11eb-8aec-dc7031a9a956.png)
 
 -> E조가 같은 한 단어이므로 B와 I가 태깅되었다.
 
 -> LSTM은 순환 신경망 모델의 일종으로 시퀀스 또는 시계열 데이터의 패턴을 인식하는 분야에 많이 사용
 
## RNN

 : 은닉층 노드의 출력값을 출력층과 그 다음 시점의 은닉층 노드의 입력으로 전달해 순환한다.
 
 ![z2](https://user-images.githubusercontent.com/59636424/115551149-9ec73d00-a2e5-11eb-9079-833e9d46541b.png)
 
 : x는 입력 벡터, y는 출력 벡터, t는 현재 시점, xt는 현재 시점의 입력 벡터, yt는 현재 시점의 출력 벡터
 
 -> RNN의 은닉층 노드는 이전 시점(t-1)의 상태값을 저장하는 메모리 역할을 수행하기에 셀 or 메모리 셀이라고 한다.
 
 -> 은닉층의 메모리 셀의 출력 벡터는 출력층과 다음 시점(t+1)의 메모리 셀에 전달(= 은닉 상태)(ht는 현재 시점의 은닉 상태)
 
 ![z3](https://user-images.githubusercontent.com/59636424/115551794-860b5700-a2e6-11eb-96ff-938de9ddbb51.png)
 
 : 현재 시점의 메모리 셀은 이전 시점의 은닉 상탯값에 영향을 받는 완전 연결 계층 구조이다.
 
 -> RNN은 어떤 문제를 해결하느냐에 따라 입력값 갯수와 출력값 갯수를 제어할 수 있다.
 
 ### 1. many-to-one
 
  : 입력값을 다수로 받고 출력값이 하나인 경우이다.
  
  -> 09:00부터 09:59까지 분 단위로 온도 데이터를 입력받아 현재까지의 온도 흐름이 정상, 비정상인지 판단할 때 사용 가능
  
  ![z4](https://user-images.githubusercontent.com/59636424/115552245-10ec5180-a2e7-11eb-8bb0-abdab6f3453b.png)
  
 ### 2. one-to-many
 
  : 입력값 하나로 다수의 출력을 나타내는 경우이다.
  
  -> 이미지를 입력받아 이미지를 설명하는 텍스트를 출력 시 사용 가능
  
  ![z5](https://user-images.githubusercontent.com/59636424/115552400-43964a00-a2e7-11eb-8fc0-055df75ab340.png)
  
 ### 3. many-to-many
 
  : 많은 입력값과 많은 출력값을 나타내는 경우이다.
  
  -> 개체명 인식기에 사용된다.(단어 시퀀스를 입력받아 각 시퀀스가 의미하는 개체명을 출력하는 구조), 번역기로 사용 가능
  
  ![z6](https://user-images.githubusercontent.com/59636424/115552623-7b9d8d00-a2e7-11eb-8434-82c3f4485628.png)
  
 **### RNN의 구조**
 
  -> RNN은 모든 시점에서 동일한 가중치와 편향값을 사용한다.
  
  ![z7](https://user-images.githubusercontent.com/59636424/115552883-bef7fb80-a2e7-11eb-82e5-029c4f93bf1a.png)
  
  -> xt는 현재 시점의 입력 벡터, yt는 현재 시점의 출력 벡터, ht는 현재 시점의 은닉 상태 벡터값을 의미
  
  -> wx는 입력 xt에 대한 가중치, wh는 이전 시점의 은닉 상탯값인 ht-1에 대한 가중치, wy는 현재 시점의 은닉 상탯값인 ht에 대한 가중치
  
  ![z8](https://user-images.githubusercontent.com/59636424/115553236-24e48300-a2e8-11eb-9eac-32ab6b062c5c.png)
  
  -> 이전 시점의 은닉 상탯값이 현재 시점의 은닉 상태에 계속해서 영향을 주기에 시퀀스 데이터의 특징을 잘 파악한다.
  
  ![z9](https://user-images.githubusercontent.com/59636424/115553688-ab996000-a2e8-11eb-9e67-b3eafd59e3b6.png)
  
  : 순환신경망은 변화하는 입력을 받기 때문에 각 단계에서 입력이 변할 때의 계산의 흐름(위의 사진은 Simple RNN 레이어)
  
  - Simple RNN 레이어 출력
  
  ![z10](https://user-images.githubusercontent.com/59636424/115553887-dedbef00-a2e8-11eb-9228-f57d53e9267b.png)
  
  **-> 여기서 활성화함수로 tanh를 사용하는데 이유는 RNN은 같은 가중치 W를 사용하기 때문이다.**
  
  **-> 같은 가중치면 미분을 사용해서 오차를 구하는데 값이 엄청나게 커지는 가중치 폭발과 음수값이 일어나면 소실 문제 발생**
  
  **-> 이러한 상황을 tanh가 극복시켜준다.**
