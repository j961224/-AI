# 자연어 처리

 : 일상에서 사용하는 언어릴 자연어라고 칭하며 컴퓨터가 이를 분석하여 처리할 수 있도록 하는 일이다.

## 토큰(token)

 : 가장 기본이 되는 단어를 칭하며 토크나이징(tokenizing) 방법에 따라 달라질 수 있지만 일반적으로 의미가 있는 가장 작은 정보 단위로 결정된다.
 
 - 토크나이징
 
  : 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업(텍스트 전처리 과정에 주로 사용)
  
  - **KoNLPy(코엔엘파이)**
  
    : 대표적인 한국어 토크나이징을 지원하는 파이썬 모듈(형태소를 토큰 단위로 사용)
  
  - 형태소
  
    : 일정한 의미가 있는 가장 작은 말의 단위(더 이상 쪼개지지 않는 단어)
    
    : 한국어는 용언에 따라 여러 가지 의미를 붙기에 띄어쓰기만으로는 토크나이징 할 수 없다.
    
    : 그래서 문장에서 형태소를 추출하면서 형태소의 뜻과 문맥을 고려해 품사 태깅(KoNLPy에서 pos가 그 역할을 한다.)
  
  - KoNLPy 패키지 이용법: https://konlpy-ko.readthedocs.io/ko/v0.4.3/

## Kkma

 : konlpy.tag 패키지 중 하나의 모듈로 자연어 처리를 위한 한국형 형태소 분석기
 
 |함수|설명|
 |----|-----|
 |morphs(phrase)|인자로 입력한 문장을 형태소 단위로 토크나안징 한다. 이들은 리스트 형태로 반환|
 |nouns(phrase)|인자로 입력한 문장에서 품사가 명사은 토큰만 추출|
 |pos(phrase,flatten=True)|POS tagger라고도 하며, 인자로 입력한 문장에서 형태소 추출 후, 품사를 태깅하여 튜플 형태로 묶어 리스트로 반환|
 |setences(phrase)|인자로 입력한 여러 문장을 분리해주는 역할, 이 또한 리스트로 변환|
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  - 주로 Kkma 사용 시, 한글 형태소 품사 태그표 주로 참조: http://kkma.snu.ac.kr/documents/?doc=postag

## Komoran
  
  : 자바로 개발한 한국어 형태소 분석기로 다른 형태소 분석기와 다르게 공백이 포함된 형태소 단위로도 분석 가능
  
  : 위의 Kkma의 morphs, nouns, pos 함수를 사용한다.
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  - Komoran은 Kkma보다 형태소를 빠르게 분석해 다양한 품사 태그를 지원한다.
   
   : https://www.shineware.co.kr/products/komoran/#demo?utm_source=komoran-kr&utm_medium=Referral&utm_campaign=github-demo
   
   (실습은 NLP_basic.ipynb에서 참조)


## Okt

  : Twitter 한국어 처리기에서 파생된 한국어 처리기로 띄어쓰기가 어느 정도 되어 있는 문장을 빠르게 분석할 때, 자주 사용된다.
  
  앞의 Komoran 패키지의 함수 3개를 지원하고 2개를 더 지원합니다.
  
  |함수|설명|
  |----|----|
  |normalize(phrase)|입력한 문장을 정규화시킨다. (정규화 이전: 사랑햌ㅋㅋ -> 정규화 이후: 사랑해ㅋㅋ|
  |phrases(phrase)|입력한 문장에서 어구룰 추출한다. (입력: 오늘 날씨가 좋아요. -> 출럭: ['오늘', '오늘 날씨','날씨'])|
  
  - Okt는 앞의 형태소 분석기보다 품사 정보는 작지만 분석 속도가 빠르고 normalize함수를 통해 오타가 섞인 문장을 정규화 해준다.
  
  (실습은 NLP_basic.ipynb에서 참조)
 
## 챗봇

  : 챗봇의 입력 데이터는 보통 **인터넷 구어체**가 많다. 따라서 새롭게 생겨나는 단어나 문장은 형태소 분석기가 인식을 못하는 경우가 많다. 그러므로 기존의 많은 문장을 기반으로 학습하여 형태소 분석기를 개발했기에 **새로운 형태의 단어와 문장은 인식률 저하의 원인**이 된다.
  
  -> 이를 해결하기 위해서 형태소 분석기에서 인식하지 못하는 단어들을 직접 추가하는 방법을 살펴본다.
  
  (실습은 NLP_basic.ipynb에서 참조)
  
  : 실습에서는 Komoran을 사용했지만 Kkma나 Okt를 사용해도 무관

## Kkma, Okt, Komoran 형태소 분석기 정리

: 이때까지 한국어 토크나이징을 알아보았다. 영어는 단순히 토큰 정보만 필요하다면 띄어쓰기만 하더라도 좋은 결과를 보여준다.

  -> 하지만 **한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러 가지 어미를 붙이기에 띄어쓰기만으로는 토크나이징 불가**
  
  -> 그래서 KoNLPy 형태소 분석기를 이용해 형태소 단위의 토큰과 품사 정보를 추출했고 이에 **필요 없는 정보를 제거하는 과정을 추가**해야한다!
  
  -> 이를 **전처리**라고 한다.
  
  ![형태소분석기](https://user-images.githubusercontent.com/59636424/115201639-84982e00-a130-11eb-8e1d-9529f05c6346.png)

## 임베딩

  : 자연어를 숫자나 벡터 형태로 변환 -> 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정
  
  - 문장 임베딩
  
   : 문장 전체를 벡터로 표현하는 방법으로 전체 문장의 흐름을 파악해 벡터로 변환하기에 문맥적 의미를 지니는 장점이 있다. 하지만 임베딩하기 위해 많은 문장 데이터가 필요하고 학습 비용이 많이 든다.
   
   - 단어 임베딩
   
   : 말뭉치에서 각각의 단어를 벡터로 변환하는 방법
   
   -> 동음 이의어에 대한 구분을 하지 않기에 의미가 다르더라도 단어 형태가 같다면 동일한 벡터값을 부여한다. (ex. 과일 사과, 미안할 때 사과)
   
   - **원-핫 인코딩**
   
   : 단어를 숫자 벡터로 변환하는 가장 기본적인 방법으로 요소들 중 단 하나의 값만 1이고 나머지 요소값은 0으로 인코딩힌다.
   
   -> 전체 요소 중 단 하나의 값만 1이기에 희소 벡터라고도 한다. (ex. ([1,0,0],[0,1,0],[0,0,1] 표현)
   
   * 원-핫 인코딩을 수행 시, 단어 집합인 **사전**을 먼저 만들어야한다. (서로 다른 모든 단어의 집합)
   
   -> 이로써, 100단어면 100차원으로 사전 내 단어 순서대로 고유한 인덱스 번호를 부여한다.
   
   (실습은 NLP_basic.ipynb에서 참조)
   
   * 희소 벡터는 각각의 차원이 독립적으로 되어 있어 직관적으로 해석이 가능하지만 메모리가 많이 사용되고 단어 간의 연관성을 알 수 없다.
   
   -> 이를 해결하기 위해서 각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 **분산 표현**을 고안했다.
   
     ex) 색상을 표현하는 방식 예시
   
      : RGB모델은 3차원 형태의 벡터로 생각할 수 있으며 분산 표현 방식
   
   -> 분산 표현 시, 임베딩 벡터의 모든 차원에 의미 있는 데이터를 고르게 밀집
   
   -> 이로써, 데이터 손실을 최소화하며 벡터 차원이 압축되는 효과가 생긴다. (여러 차원에 분산되어 표현)
   
   -> 원하는 차원에 데이터를 최대한 밀집시킬 수 있는 **밀집 표현**으로 벡터 압축 효과가 생긴다.
   
   * 단어 많으면 회소 표현 방식을 사용 할 시, 너무 많은 차원이 필요하고 비효율적이게 되어 학습이 어려워지는 **차원의 저주** 문제가 발생한다.
   
     -> 분산 표현 방식으로 임베딩 벡터 차원을 데이터 손실 최소화시키므로 차원의 저주 발생 확률을 줄인다.
   
   * 임베딩 벡터는 단어의 의미, 주변 단어간의 관계 등 많은 정보가 표현되어 있어 일반화 능력이 존재한다.
   
   ex) 희소 표현 방식 시, '남자'와 '남성'의 관계는 전혀 없다. 하지만 분산 표현 방식 사용 시, 유사한 의미를 갖는 단어들은 '남자'와 '남성'의 단어 위츠는 매우 가깝게 표현된다.
   
   ![분산표현기법장점1](https://user-images.githubusercontent.com/59636424/115204615-a5ae4e00-a133-11eb-855c-cb8bdb1e2186.png)

## Word2Vec

  : 대표적인 단어 임베딩 기법
  
  -> 해당 단어를 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치
  
  -> 방향성에 대해 이해와 연산을 통해 단어간 관계를 계산 할 수 있다.
  
  1. CBOW(continuous bag-of words)모델
  
   : 맥락이라 표현되는 주변 단어(앞뒤 단어)들을 이용해 타겟 단어를 예측하는 신경망 모델
   
   -> 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용
   
   -> 손실만 계산하면 되므로 학습 속도가 빠르다.
   
   * Window: 앞뒤로 몇 개의 단어까지 확인할지 결정
  
  2. skip-gram 모델
  
   : 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델
   
   -> CBOW모델에 비해 예측해야 하는 맥락이 많다.
   
   -> CBOW모델에 비해 임베딩 품질이 우수
   
   -> 확률 분포를 가지고 주변 단어를 예측
  
 - 임베딩 모델링과 자연어 처리를 위한 Gensim 패키지 주로 사용
  
  (ex. Movie_review_Word2Vec.ipynb)
    
