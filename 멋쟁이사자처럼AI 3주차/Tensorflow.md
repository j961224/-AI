# 3주차 내용

## Tensorflow 기본
  : 머신러닝을 위한 오픈소스 플랫폼
  
  - 신경망에 난수가 중요! -> 이유는?
    : 신경망을 쉽게 보면 많은 숫자로 구성된 행렬이고 신경망의 초깃값을 지정해야하므로 중요!
    (Xavier 초기화와 He 초기화가 있음)
    -> 어느 정도 규칙성이 있는 범위 내에서 난수를 지정한다.
    
  - uniform함수를 통한 균일 분포 난수 얻음
    -> 균일 분포란 최솟값과 최댓값 사이의 모든 수가 나올 확률이 동일한 분포에 수를 뽑음
  
  - 정규 분포: 난수를 얻는 다른 방법으로 양근단으로 갈수록 낮아지는 종 모양을 그리는 분포
    (정규 분포의 평균이 0, 표준 편차가 1이라면 표준정규분포라고 한다.)
    
  - Xavier 초기화, He 초기화는 균일 분포나 정규 분포를 선택하여 신경망의 초기값을 초기화한다.

  - 뉴런(입력, 가중치, 활성화 함수, 출력으로 구성)
    : 퍼셉트론이라고도 하며 입력을 받아 계산하여 출력을 하는 단순한 구조
    - 입력, 가중치, 출력은 정수나 실수로 구성
    - 활성화 함수는 뉴런의 출력값을 정하는 함수(보통 시그모이드(S자 곡선)와 ReLU(정류된 선형 함수) 함수가 있다.)
  
  - 딥러닝에서의 오류로 역전파할 때, 시그모이드 함수가 값을 점점 작아지게 하는 문제로 ReLU 등장
    (관련 논문: https://www.cs.toronto.edu/~fritz/absps/relulCML.pdf)
    
  - 신경망
    : 뉴런 여러개가 모여 layer를 구성한 후에 layer가 다시 모여 구성된 형태
  
  - 학습이 잘 된다는 것은 좋은 가중치를 얻어 원하는 출력으로 점점 가까운 값을 얻는 것
  
  - *경사하강법*
    : error값을 0에 가까워지게 하기 위해 가중치에 입력과 학습률과 에러를 곱한 값을 더해주는 방법
    (경사는 손실 곡선의 기울기를 뜻한다. -> 가중치가 손실이 가장 낮아지는 지점 도달하는 것이 목표)
    
  - 편향
    : 입력으로는 한쪽으로 치우친 고정된 값을 받아서 입력으로 0을 받았을 때, 뉴런이 아무 것도 못하는 상황을 방지
  
  - AND,OR,XOR 연산: 입력 2개와 편향 1개
    (여기서 XOR은 여러 개의 퍼셉트론을 사용해야 한다.)
    
  - tf.keras를 사용한 신경망 구축
    * tf.keras.Sequential: 순차적인 뉴런과 뉴런이 합쳐진 단위(레이어를 일직선으로 배치)
    * Dense를 추가하여 layer정의
    * units: 레이어를 구성하는 뉴런의 수
    * input_shape: 첫 번째 layer에서만 정의하는데 입력 차원 수가 어떻게 되는지 정의
  
  - compile 할 때
    * optimizer: 딥러닝의 학습식을 정의
    * SGD: 확률적 경사 하강법으로 경사 하강법을 확률적으로 일부 샘플을 구해서 조금씩 나눠서 계산
    * loss에는 보통 평가 지표를 넣는다. 만약 mse를 넣었다면 평균 제곱 오차를 뜻합니다.
  
  - fit 할 때
    * epoch: 훈련 데이터 반복시키는 횟수
    * batch_size: 한 번에 학습시키는 데이터 수
  
  ex) XOR을 위한 layer 구축
  ![이번꺼](https://user-images.githubusercontent.com/59636424/112135321-00c25300-8c11-11eb-8265-1664473e021c.png)
  
  - 선형 회귀
  
    : 데이터의 경향성을 가장 잘 설명하는 하나의 직선을 예측
    
    (ex. 국어 성적과 수학 성적, 키와 몸무게, 치킨 판매량과 맥주 판매량 등의 2개의 데이터에 대한 경향성 예측)
    
    ex. 2018년 지역별 인구 증갸율과 고령인구비율 경향성을 선형회귀로 예측 -> RegionbyOld2018.ipynb
    
    ex. 회귀를 통해 보스턴 주택 가격 예측 -> Boston_house_price_prediction.ipynb
    
    * 잔차: 경향성을 가장 잘 설명하는 하나의 직선과 각 데이터의 차이
    * LSM: 잔차의 제곱을 최소화하는 알고리즘을 최소제곱법이라 한다.
    
    
    
