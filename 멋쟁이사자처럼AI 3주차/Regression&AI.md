# 3주차 내용

## 인공지능과 머신러닝, 딥러닝(인공지능>머신러닝>딥러닝)
  - 인공지능

    : 사람이 해야 할일을 가져가 대신할 수 있는 모든 자동화에 해당
    
  - 머신러닝
  
    : 명시적으로 규칙을 프로그래밍하지 않고 데이터로부터 의사결정을 위한 패턴을 기계가 스스로 학습
    
  - 딥러닝
    
    : 인공신경망 기반의 모델로, 비정형 데이터로부터 특징 추출 및 판단까지 기계가 한 번에 수행
    
  - 학습

   : 데이터로부터 규칙이나 패턴을 획득하는 과정 -> 이것을 프로그램이 직접 수행하여 머신러닝이라 한다.
   
  - 인공신경망
  
   : 뉴런을 흉내 내어 고안한 퍼셉트론 단위로 구성
    
  - 합성함수
  
   : 머신러닝에서 복잡한 함수를 미분하는 경우에 해당 함수가 여러 개의 단순한 함수로 구성된 합성함수라고 간주하고 계산하여 비교적 쉽게 미분
    

  - 단층 퍼셉트론(오직 출력 계층 하나만 구성)
  
   : 일련의 퍼셉트론을 한 줄로 배치하여 입력 벡터 하나로부터 출력 벡터 하나를 단번에 얻어내는 가장 기본적인 신경망 구조
   
   -> 출력 벡터가 담고 있어야 할 스칼라 성분의 수 만큼의 퍼셉트론 필요
   (퍼셉트론마다 가중치 벡터와 편향값을 이용하여 입력 벡터 x로부터 출력 벡터 y를 도출)
   
   -> 퍼셉트론의 가중치 벡터들을 한데 모은 것이 가중치 행렬 W, 편향값을 한데 모은 것이 편향 벡터 b(이러한 가중치와 편향이 파라미터)
  
 - 텐서

  : 다차원 숫자 배열로 텐서가 중요한 이유는 반복문 대신 텐서를 이용해 처리하는 편이 프로그램도 간단하고 처리 속도도 빠르다.
  
  -> 신경망이 여러 데이터를 한 번에 처리하는 경우에 미니배치라고 한다.
  
 - 미니배치
 
  : 배치 작업보다 상대적으로 작은 단위로 처리하는 일괄처리
  
  -> 데이터 처리의 효율을 높여주며 개별 학습 데이터의 특징을 무시하지 않으면서 너무 휘둘리지 않게 해줘서 유용
  
 - 하이퍼파라미터
 
  : 설계자가 학습 전에 미리 정해주는 값으로 학습결과에 큰 영향을 준다.(에폭 수나 미니배치와 같이 크기가 변경되지 않는 것)
  
  
## 신경망의 3가지 유형
  1. 회귀 분석
  
    : 어떤 특정값 하나를 숫자로 추정하여 출력
    
  2. 이진 판단

    : 예, 아니오 가운데 한 쪽을 택해 출력
  
  3. 선택 분류
  
    : 몇 가지 후보 중 하나를 골라 선택 결과 출력
   
   
## 회귀 분석
  : 연속형 변수 사이의 모형을 구한 뒤 적합도를 측정하는 방법, 입력으로 주어진 값들을 근거로 미지의 변수값을 추정하고 예측하는데 주로 사용(상관관계 찾기)
  
  - 최소 제곱법
  
    : 모든 점에 생기는 오차의 합계가 가능한 작아지도록 한다.
    
   배경) 예를 들어 y=1+2x라는 1이 편향, 2가 기울기인 식으로 광고비 데이터를 대입해 클릭 수 예측값과 실제값의 차이가 최소가 되도록 '세타' 정하기
   
   -> 오차가 없는 것이 이상적이지만 0으로 만드는 것은 불가능하다.
   
   -> 그러므로 오차의 합계를 가능한 작도록 한다.
   
   식)
   
   ![이번꺼](https://user-images.githubusercontent.com/59636424/112146740-47b74500-8c1f-11eb-9130-ca3bb10ac91f.png)
   
   : 목적함수 E(세타)로서 각각의 데이터에 생기는 오차를 제곱하여 더해 1/2를 곱하였다.
   
   -> 오차 제곱의 이유는 오차가 +,- 둘다 생겨 합이 0으로 발생할 경우를 막기 위해서
   
   -> 1/2를 곱하는 이유는 미분으로 인해 발생하는 문제를 막기 위해서
    
  - 경사하강법
  
    -> 목적함수를 감소시키기 위해서 매개변수 세터를 적절히 조절해야하는데 미분이 사용된다.
    
     (변화하는 정도 측정을 위해서)
     
    : 에타를 '학습률'이라는 양의 정수 사용하여 학습률에 따라 최솟값 도달하기까지 갱신해야 하는 횟수가 달라지는데 이에 최솟값에 수렴 속도가 달라진다.
     
     -> 그러나 최솟값에 수렴않고 발산할 수 있다.
  ![이번꺼1](https://user-images.githubusercontent.com/59636424/112154440-9ff24500-8c27-11eb-998d-bfcc6d9bda91.png)   
  
  -> 무작정 차수를 늘리는 방식은 과적합에 걸린다.
  
  - 다항식 회귀
  
   : 매개변수를 늘려 다항식의 차수를 늘린 함수를 사용
   
  - 다중 회귀
  
   : 여러개의 변수를 사용하는 것
   
  - 확률적 경사하강법(적절한 학습률이 중요)
  
   : 모든 학습 데이터의 오차를 사용하는데 학습 데이터를 무작위로 한 개 골라서 그것을 매개변수 갱신에 사용
   
   -> 최소 제곱법이 힘든 이유는 매번 초기값이 변하면 local minimum이 빠지게 되므로 사용이 힘듦니다.
   
   -> 확률적 경사하강법을 무작위로 m개 선택해 매개변수 갱신하는 방법도 있다.
   
   (이러한 방식은 최소 제곱법과 확률적 경사하강법의 중간을 취하여서 미니배치 경사하강법이라고도 한다.)
  
  - 평균제곱오차
  
   : 각 출력 성분에 대한 추정값과 실제값 사이에 오차를 제곱하여 합하여 전체 성분 수로 나눈 값
   
   (회귀분석에서 추정값이 얼마나 정확한지에 대한 평가지표로 씀) -> 0에 가까울수록 오차가 적다.
  
     
     
   
