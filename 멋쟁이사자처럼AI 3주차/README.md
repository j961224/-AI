# 3주차 내용

## Tensorflow 기본
  : 머신러닝을 위한 오픈소스 플랫폼
  
  - 신경망에 난수가 중요! -> 이유는?
    : 신경망을 쉽게 보면 많은 숫자로 구성된 행렬이고 신경망의 초깃값을 지정해야하므로 중요!
    (Xavier 초기화와 He 초기화가 있음)
    -> 어느 정도 규칙성이 있는 범위 내에서 난수를 지정한다.
    
  - uniform함수를 통한 균일 분포 난수 얻음
    -> 균일 분포란 최솟값과 최댓값 사이의 모든 수가 나올 확률이 동일한 분포에 수를 뽑음
  
  - 정규 분포: 난수를 얻는 다른 방법으로 양근단으로 갈수록 낮아지는 종 모양을 그리는 분포
    (정규 분포의 평균이 0, 표준 편차가 1이라면 표준정규분포라고 한다.)
    
  - Xavier 초기화, He 초기화는 균일 분포나 정규 분포를 선택하여 신경망의 초기값을 초기화한다.

  - 뉴런(입력, 가중치, 활성화 함수, 출력으로 구성)
    : 퍼셉트론이라고도 하며 입력을 받아 계산하여 출력을 하는 단순한 구조
    - 입력, 가중치, 출력은 정수나 실수로 구성
    - 활성화 함수는 뉴런의 출력값을 정하는 함수(보통 시그모이드(S자 곡선)와 ReLU(정류된 선형 함수) 함수가 있다.)
  
  - 딥러닝에서의 오류로 역전파할 때, 시그모이드 함수가 값을 점점 작아지게 하는 문제로 ReLU 등장
    (관련 논문: https://www.cs.toronto.edu/~fritz/absps/relulCML.pdf)
    
  - 신경망
    : 뉴런 여러개가 모여 layer를 구성한 후에 layer가 다시 모여 구성된 형태
  
  - 학습이 잘 된다는 것은 좋은 가중치를 얻어 원하는 출력으로 점점 가까운 값을 얻는 것
  
  - *경사하강법*
    : error값을 0에 가까워지게 하기 위해 가중치에 입력과 학습률과 에러를 곱한 값을 더해주는 방법
    (경사는 손실 곡선의 기울기를 뜻한다. -> 가중치가 손실이 가장 낮아지는 지점 도달하는 것이 목표)
    
  - 편향
    : 입력으로는 한쪽으로 치우친 고정된 값을 받아서 입력으로 0을 받았을 때, 뉴런이 아무 것도 못하는 상황을 방지
  
  - AND,OR,XOR 연산: 입력 2개와 편향 1개
    
    
