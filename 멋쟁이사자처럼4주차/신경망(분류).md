# Sigmoid 함수

 : 선형 연산에서는 일단 범위에 제한이 없는 실숫값을 계산하되, 이를 확률값의 성질에 맞게 즉 0~1 사이의 값을 갖도록 변환해주는 비선형 활성화 함수
 
## Cross Entropy
  
 : 이진 판단 문제의 손실함수로 두가지 확률 분포로 **신경망의 예측값에 따른 확률 분포**와 **실제 결괏값에 따른 확률 분포** 사이의 각 확률 분포가 얼마나 다른지 표현(**두 확률분포가 서로 얼마나 다른지 나타내는 정량적 지표 역할 수행**)
 
 ![교차 엔프로피](https://user-images.githubusercontent.com/59636424/112996186-ea833c80-91a6-11eb-8bd7-cd9bb3eabecc.png)
 
 -> 정보량을 제공하는 확률분포(-logpi)와 가중평균 계산에 사용되는 확률분포(pi)를 다르게 설정하여 정보량의 기댓값을 구하는 방식

 -> **틀릴 수 있는 정보를 가지고 구한 엔트로피** 즉 정보량이다.
 
 -> 딥러닝의 예측이 완전히 빗나가게 되면 교차 엔프로피 값은 무한대가 되지만 어느정도 학습되면 그 값은 엔프로피 값으로 수렴
 
## Sigmoid 함수의 정의

 ![sigmoid 정의](https://user-images.githubusercontent.com/59636424/112951458-56997c80-9176-11eb-9a81-554c3d0bf4e0.png)
 
 : 이 함수 그대로 쓰면 종종 오버플로우 문제가 발생한다!(x에 큰 음수가 들어올 경우)
 
 - 해결법!
 ![sigmoid 정의1](https://user-images.githubusercontent.com/59636424/112956322-4df77500-917b-11eb-81dc-04a9c00eafc1.png)
 
## Entropy

  : 확률 분포의 무질서도, 불확실성이나 정보 표현의 부담 정도
  
  ## 정보이론
  
   : 신호에 존재하는 정보의 양을 촉정하는 이론
    
   - Pi의 역수 관계를 이용하여 정보 엔프로피(정보량의 평균 - H)표현
    
   ![정보이용량](https://user-images.githubusercontent.com/59636424/112953370-65812e80-9178-11eb-8ff1-ee4d1bf3381f.png)
    
   : 정보 엔프로피는 어떤 확률 분포로 일어나는 사건을 표현하는데 필요한 정보량으로 표현한 것으로 **확률분포의 불확실성**을 뜻합니다.
  
  ## 시그모이드 함수의 교차 엔프로피 식(함수)
  
   -> 시그모이드와 시그모이드 교차 엔트로피 식에 음수나 양수의 입력값 모두 처리할 수 있는 하나의 식 필요
    
   (위의 sigmoid 함수에서 x가 음수일 때 발생하는 문제를 해결 -> 오버플로우 문제 해결)
    
   ![sigmoid 정의2](https://user-images.githubusercontent.com/59636424/112957324-497f8c00-917c-11eb-9c61-8e3587b225c5.png)
    
   ex) Pulsar Star 예측하는 문제
    
  ## 평가 지표
  
  - Accuracy의 한계로 인해 평가 지표
    
    1. 정밀도(precision)
    
      : 참으로 예측한 것중 정답이 참인 비율 ( TP/(TP+FP) )
    
    2. 재현율(recall)
    
      : 정답이 참인 것들 중에 참으로 예측한 비율 ( TP/(TP+FN) )
   
    -> 하지만 정밀도와 재현율 중 하나의 값 올리기 또한 쉽다.
    
    3. F1-score
      
      : 정밀도와 재현율의 조화평균
      
      ![f1 score](https://user-images.githubusercontent.com/59636424/112973306-2361e800-918c-11eb-8f01-7d17f2cbb266.png)
      
      
   
 
 
